{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><img src=\"media/imagenes/Banner.png\" height = 100 width = 1000></center>",
   "id": "d962182cfdf723d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Este notebooj pretende enseñar el resultadod de todas las implementaciones sin meterse mucho en los tedalles de la implementación, pretende ser una guia visual e interactiva, para endender todas las decisiones de diseño que hemos tomado en el proyecto. Esta pensado para ser ejecutado celda a celda salvo que se diga lo contrario.",
   "id": "e0375d750347b858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:44:23.874954Z",
     "start_time": "2025-01-17T21:44:21.429653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dependencias\n",
    "from pprint import pprint\n",
    "\n",
    "from dependencias import enfrentar"
   ],
   "id": "111b71fb835de854",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reglas\n",
    "\n",
    "Hay que enseñar lo que hemos implentado del juego base"
   ],
   "id": "22f54753c31fe610"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agentes",
   "id": "bc289102eebca739"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"media/imagenes/euristicos.png\" style=\"float: right;\">\n",
    "\n",
    "### Agentes Basados en Heuristicas.\n",
    "\n",
    "Los agentes que vamos a enseñar en este apartado son agentes que se basan en una politica de recompensa para decidir que movimiento realizar en cada turno, estan implementados mediante una clase abstracta que tiene los siguientes metodos:\n",
    "\n",
    "- get_reward: Calcula la recompensa asociada a cada movimiento en funcion de una politica de recompensa.\n",
    "- get_action: Calcula la mejor acción a realizar en función de la recompensa obtenida.\n",
    "\n",
    "de este modo podemos empezar a jugar con el agente y ver como se comporta.\n",
    "\n"
   ],
   "id": "4ebbbd6af6c0c68d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuacion vamos a enseñar los agentes que hemos implementado y como se comportan en el juego.",
   "id": "7acd514c49b15b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Chaser\n",
    "\n",
    "La politica de recompensa de este agente se basa en la distancia entre la cabeza de la serpiente y la comida mas cercana, de este modo el agente siempre intentará acercarse a la comida, aunque esto implique que la serpiente se muera.\n",
    "\n",
    "<center><img src=\"media/imagenes/chasser.png\" style=\"float: center;\"></center>"
   ],
   "id": "3e9d7108e737e3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:44.036645Z",
     "start_time": "2025-01-17T21:42:44.033114Z"
    }
   },
   "cell_type": "code",
   "source": "agente_chaser = dependencias.Agentes.ChaserAgent()",
   "id": "ce71e919a9774741",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:45.426395Z",
     "start_time": "2025-01-17T21:42:44.049358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente puedes modificar los parametros del juego para ver como se comporta en distintos escenarios\n",
    "\n",
    "SIZE = (15, 15)\n",
    "N_FOODS = 5\n",
    "\n",
    "game = dependencias.Snake_game(SIZE, N_FOODS, agente_chaser)\n",
    "game.play_with_pygame()"
   ],
   "id": "8b93e851f276b71d",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:45.525765Z",
     "start_time": "2025-01-17T21:42:45.453560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ],
   "id": "a40361a9bfbb8600",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1522.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': 4,\n",
      " 'movimientos_maximos': np.int64(115),\n",
      " 'movimientos_medios': np.float64(29.64),\n",
      " 'movimientos_minimos': np.int64(6),\n",
      " 'proporcion_del_tablero_ocupada': 0.01,\n",
      " 'puntuacion_maxima': np.int64(23),\n",
      " 'puntuacion_media': np.float64(6.81),\n",
      " 'puntuacion_minima': np.int64(3)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "Este agente logra su objetivo de llegar muy rapido a por la comida (movimientos por puntuacion), pero no consigue sobrevivir mucho tiempo.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "Este agente podría ser mejorado si se le añade una penalización por muerte inmediata."
   ],
   "id": "982affadbdb6076b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Avoider\n",
    "\n",
    "La politica de recompensa de este agente se basa en evitar las muertes inediatas, ya sea contra la serpriente o contra las paredes, asociando una penalización a estos movimientos.\n",
    "\n",
    "<center><img src=\"media/imagenes/inminent.png\" style=\"float: center;\"></center>"
   ],
   "id": "5895a96839b301c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:45.624309Z",
     "start_time": "2025-01-17T21:42:45.620528Z"
    }
   },
   "cell_type": "code",
   "source": "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()",
   "id": "47d1db2bdeefc94d",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-17T21:42:45.636194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "SIZE = (15, 15)\n",
    "N_FOODS = 5\n",
    "\n",
    "game = dependencias.Snake_game(SIZE, N_FOODS, agente_avoider)\n",
    "game.play_with_pygame()"
   ],
   "id": "34242dee54cd21fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.104081400Z",
     "start_time": "2025-01-17T19:52:32.178081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ],
   "id": "3161d72e0bd1f13b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 367.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': 63,\n",
      " 'movimientos_maximos': np.int64(1379),\n",
      " 'movimientos_medios': np.float64(634.02),\n",
      " 'movimientos_minimos': np.int64(175),\n",
      " 'proporcion_del_tablero_ocupada': 0.04,\n",
      " 'puntuacion_maxima': np.int64(17),\n",
      " 'puntuacion_media': np.float64(10.13),\n",
      " 'puntuacion_minima': np.int64(5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "Este agente majora mucho su tiempo de vida, pero no consigue alcanzar la comida de manera eficiente.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "Ademas vemos que con evitar muertes inmediatas no es suficiente, puesto que hay escenarios en los que te puedes quedar encerrado."
   ],
   "id": "4ce8684424ccb852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Combinacion de agentes\n",
    "\n",
    "Esta implementation de los agentes tan particular, nos permite combinar las recompensas que le dan a cada movimiento distintos agentes para conseguir un agente que se comporte de una manera más eficiente.\n",
    "\n",
    "<img src=\"media/imagenes/combinado.png\" style=\"display: block; margin: auto;\" width=\"400\">\n",
    "\n"
   ],
   "id": "b27f538b5d98c989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.105588700Z",
     "start_time": "2025-01-17T19:52:34.415019Z"
    }
   },
   "cell_type": "code",
   "source": "chouder = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser), (1, 0.5))",
   "id": "26314a8eefae079a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.105588700Z",
     "start_time": "2025-01-17T19:52:34.841726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "game = dependencias.Snake_game((15, 15), 5, chouder)\n",
    "game.play_with_pygame()"
   ],
   "id": "fed9b2ad778e7e8f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.105588700Z",
     "start_time": "2025-01-17T19:52:37.266080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ],
   "id": "d6df7ca7bbbc7ec1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 207.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': 5,\n",
      " 'movimientos_maximos': np.int64(401),\n",
      " 'movimientos_medios': np.float64(179.84),\n",
      " 'movimientos_minimos': np.int64(34),\n",
      " 'proporcion_del_tablero_ocupada': 0.14,\n",
      " 'puntuacion_maxima': np.int64(73),\n",
      " 'puntuacion_media': np.float64(33.59),\n",
      " 'puntuacion_minima': np.int64(10)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "El comportaiento de este agente es mucho mejor que el de los agentes individuales, y parece una buena estrategia para el inicio del juego.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "El agente sigue sin ser capaz de evitar quedarse encerrado, hay que añadir una heuristica mas a futuro para evitar este tipo de situaciones.\n",
    "\n",
    "<img src=\"media/imagenes/encerrado.png\" style=\"display: block; margin: auto;\" width=\"400\">\n",
    "\n",
    "*en el ejemplo moverse a la derecha provoca una muerte inevitable pero el agente no lo considera una mala jugada*"
   ],
   "id": "b6b8c6818edb8ffc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Politicas basadas en busqueda en anchura (Estrategias a largo plazo)",
   "id": "31cd4850e427b219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "para hacer decisiones a largo plazo se nos plantean dos opciones.\n",
    "\n",
    "- Fuerza bruta (Exploracion en estados): podemos ver a donde nos lleva una combinacion de movimientos y ver cual es mejor.\n",
    "\n",
    "problema: el espacio de estados es muy grande y no podemos explorar todas las posibilidades, tendriamos que seleccionar una profundidad maxima y explorar hasta ahi. Además relentizaria cada simulacion.\n",
    "\n",
    "- Busqueda en anchura: Lanzando busqueda en anchura desde los movimientos posibles en cada turno podemos ver cual es el mejor movimiento a largo plazo, evaluando algunas caracteristicas del tablero.\n",
    "\n",
    "A continuacion desarrollamos agentes basados en busqueda en anchura, para solucionar probleas de muerte a largo plazo."
   ],
   "id": "6f563d3c81d96838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Searcher\n",
    "\n",
    "Este agente lanza 3 busquedas en anchura, empezando en cada una las casillas accedibles en el siguiente movimiento, aprovecha para contar el numer de casillas accesibles en cada caso y asigna una puntuacion a cada movimiento en funcion de este valor.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "$$\n",
    "W = \\frac{\\text{casillas accesibles en el movimiento } m}{\\text{total casillas accesibles}}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "*De este modo premiamos los movimientos que te llevan a los espacios abiertos*\n"
   ],
   "id": "35d3da55534f9a1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"<img src=\"media/imagenes/searcher.png\" style=\"display: block; margin: auto;\" width=\"400\">\" style=\"display: block; margin: auto;\" width=\"400\">",
   "id": "f1e5469e735379d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.105588700Z",
     "start_time": "2025-01-17T20:12:40.871292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_searcher)"
   ],
   "id": "a07b8be939832686",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.105588700Z",
     "start_time": "2025-01-17T19:52:48.002907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "ef29910c0ec2747a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuacion definimos una mezcla de los 3 agentes anteriores, para ver como se comporta en el juego.",
   "id": "586ee29055397e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.105588700Z",
     "start_time": "2025-01-17T21:13:30.846519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "agente_chaser = dependencias.Agentes.ChaserAgent()\n",
    "\n",
    "deep_chouder = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser, agente_searcher), (1, 0.2, 3))\n",
    "game = dependencias.Snake_game((15, 15), 5, deep_chouder)"
   ],
   "id": "bff528cebd91a5c6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T20:00:32.876652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "3603921eb0d7aed3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Comportamiento del agente\n",
    "\n",
    "- El comportamiento de este agente es muy bueno, consigue sobrevivir mucho tiempo y llegar a la comida de manera eficiente, además cuando se queda encerrado sigue opcupando espacio hasta generar una salida y una vez existe es capaz de escapar generando muchas huidas interesantes."
   ],
   "id": "685473e6676d911f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T19:52:52.778969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lo comparamos con el agente que desarrollamos antes.\n",
    "# Esta celda puede tardar un poco en ejecutarse se recomienda siplemente mirar los resultados.\n",
    "\n",
    "dependencias.enfrentar(deep_chouder, chouder, n_partidas=100)"
   ],
   "id": "c16e1f8b7580c4ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 218.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Estadisticas          |    Agente 1     |    Agente 2    \n",
      "------------------------------ | --------------- | ---------------\n",
      "puntuacion_media               |   ((*78.64*))   |      32.42     \n",
      "puntuacion_maxima              |    ((*119*))    |       61       \n",
      "puntuacion_minima              |    ((*50*))     |       10       \n",
      "movimientos_medios             |  ((*544.43*))   |     175.77     \n",
      "movimientos_maximos            |   ((*1038*))    |       355      \n",
      "movimientos_minimos            |    ((*325*))    |       40       \n",
      "movimientos por puntuacion     |        7        |     ((*5*))    \n",
      "proporcion_del_tablero_ocupada |   ((*0.27*))    |      0.16      \n",
      "------------------------------ | --------------- | ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Estas estaditicas muestran que deep_chouder es un agente mucho mejor que chouder, consigue sobrevivir mucho mas tiempo y llegar a mas comida. Sin embargo logicamente chouder es as eficiente y crece mas rapido.",
   "id": "59d939c67c157f79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "deep_chouder es sustancialente mejor que chouder, sin cuando la partida alcanza una catidad alta de comida, surge un problema. Como todo esta tan lleno, el agente prefiere quedarse encerrado cuando en realidad claramente lo lleva a morir.\n",
    "\n",
    "A continuacion vamos a desarrollar con busqueda en anchuera que cumpla el mismo objetivo que este a partir de otra idea que hemos tenido."
   ],
   "id": "39a21b79f91029a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Tail Chasser\n",
    "\n",
    "Este agente se basa en la idea de que si el agente tiene acceso a la cola entonces puede crear un ciclo, de modo que el agente mientras no se salga del ciclo puede seguir vivo infinitamente. Parece que encontrar coida en este ciclo da problemas, pero no es el caso, para el caso mas extremo en el que tienes una comida y inmediatamente despues la cola, el agente come y se pega a la cola, pero no llega nunca a chocarse puesto que ocupar el espacio que ocupaba la cola en el espacio anterior es un movimiento valido. (El agente presente este comportamiento en la practica muchas veces), Etonces lanzareos busqueda en anchura y premiaremos los movimientos que nos permitan llegar a la cola."
   ],
   "id": "46f4c21264c24e32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display: flex; justify-content: center; gap: 10px;\">\n",
    "    <img src=\"media/imagenes/aclaracion.png\" width=\"300\">\n",
    "    <img src=\"media/imagenes/tail_chasser.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "- *aclarar que en la imagen 1 el movimiento arriba es valido ya que la cola tambien se mueve*\n",
    "*Vemos como si tienes acceso a la cola entonces no te puedes quedar encerrado*"
   ],
   "id": "385a2b2e7323ffe4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T20:46:06.517628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tail_chasser = dependencias.Agentes.Tail_Chasser()\n",
    "game = dependencias.Snake_game((15, 15), 5, tail_chasser)"
   ],
   "id": "dc02a16d6ec00f6e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T20:46:08.025121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame() # Esta estrategia solo se puede ver si la serpiente es suficientemente larga por lo que tardara en mostrar coportamientos interesantes por si sola"
   ],
   "id": "f7c8472909e84b64",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "vamos a incorporar esta estrategia a deep_chouder para ver como se comporta.",
   "id": "413c962dc7316050"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:45:03.183976Z",
     "start_time": "2025-01-17T21:45:03.178975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "agente_chaser = dependencias.Agentes.ChaserAgent()\n",
    "agente_tail_chasser = dependencias.Agentes.Tail_Chasser()\n",
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "\n",
    "deep_loopy_looper = dependencias.Agentes.Combined_agent((agente_avoider,\n",
    "                                                         agente_chaser,\n",
    "                                                         agente_searcher,\n",
    "                                                         agente_tail_chasser),\n",
    "                                                        \n",
    "                                                        (1, 0.2, 2, 1))\n",
    "\n",
    "game = dependencias.Snake_game((15, 15), 5, deep_loopy_looper)"
   ],
   "id": "e06bd650f94b4bba",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T21:22:07.722995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "9bc21b06be5496ea",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T21:14:33.341850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dependencias.enfrentar(deep_loopy_looper, deep_chouder, n_partidas=100)\n",
    "\n",
    "# Esta celda tara un poco en ejecutar, se recomienda simplemente mirar los resultados."
   ],
   "id": "fe895e7fab47d529",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:02<00:00,  1.22s/it]\n",
      "100%|██████████| 100/100 [01:12<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Estadisticas          |    Agente 1     |    Agente 2    \n",
      "------------------------------ | --------------- | ---------------\n",
      "puntuacion_media               |   ((*88.42*))   |      79.88     \n",
      "puntuacion_maxima              |    ((*140*))    |       114      \n",
      "puntuacion_minima              |       52        |    ((*53*))    \n",
      "movimientos_medios             |  ((*633.37*))   |     548.53     \n",
      "movimientos_maximos            |   ((*1206*))    |       903      \n",
      "movimientos_minimos            |    ((*346*))    |       343      \n",
      "movimientos por puntuacion     |        7        |     ((*7*))    \n",
      "proporcion_del_tablero_ocupada |   ((*0.44*))    |      0.24      \n",
      "------------------------------ | --------------- | ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7d557a05364a162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Deep_loopy_looper se coloca se coloca en cabeza puesto que es en general mejor que deep_chouder.",
   "id": "65f00efe33039eef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Observacion\n",
    "\n",
    "*Este tipo de agentes parece que mueren consistentemente cuando desovedecen la politica de tail_chasser, parece buena idea darle mucho peso, sin embargo esto puede dar lugar a bucles infinitos, que hasta hora no estan gestionados.*\n",
    "\n",
    "\n",
    "<img src=\"media/imagenes/vacio.png\" style=\"display: block; margin: auto;\" width=\"200\">\n",
    "\n",
    "*Además este agente recubre aproximadamente la mitad de del tablero pero la eficiencia de este empaquetando es bastante mala, quizás podramos implementar una politica rellene mejor los espacios*"
   ],
   "id": "5fd2163f36db5545"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Filler\n",
    "\n",
    "Esta politica, se basa en premiar los movimientos que tangan partes de la serpiente adyacentes, de este modo se elegiran menos movimientos que separan a la serpiente demasiado a no ser que esto este justificado por otras politicas.\n",
    "\n"
   ],
   "id": "b5d330b2492bba85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:47.106597300Z",
     "start_time": "2025-01-17T21:31:35.447748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filler = dependencias.Agentes.Filler()\n",
    "\n",
    "# No tiene mucho sentido a estas alturas probar los agentes por separado, vamos a generan una cobinacion."
   ],
   "id": "6e1ca3cbb0afb2d8",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:42:54.705405Z",
     "start_time": "2025-01-17T21:42:54.695853Z"
    }
   },
   "cell_type": "code",
   "source": "import dependencias",
   "id": "fd02c14dd4166a84",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '{' on line 258 (juego_base.py, line 266)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001B[1;36m(most recent call last)\u001B[0m:\n",
      "\u001B[0m  File \u001B[0;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001B[0m in \u001B[0;35mrun_code\u001B[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001B[0m\n",
      "\u001B[0m  Cell \u001B[0;32mIn[1], line 1\u001B[0m\n    import dependencias\u001B[0m\n",
      "\u001B[1;36m  File \u001B[1;32m~\\Desktop\\test\\practica_3_ALN\\dependencias\\__init__.py:1\u001B[1;36m\n\u001B[1;33m    from dependencias.juego_base import *\u001B[1;36m\n",
      "\u001B[1;36m  File \u001B[1;32m~\\Desktop\\test\\practica_3_ALN\\dependencias\\juego_base.py:266\u001B[1;36m\u001B[0m\n\u001B[1;33m    \"proporcion_del_tablero_ocupada\" : round(np.mean(puntuaciones), 2) / (self.state.shape[0] * self.state.shape[1]), 2)\u001B[0m\n\u001B[1;37m                                                                                                                       ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m closing parenthesis ')' does not match opening parenthesis '{' on line 258\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:45:06.186436Z",
     "start_time": "2025-01-17T21:45:06.182382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "agente_chaser = dependencias.Agentes.ChaserAgent()\n",
    "agente_tail_chasser = dependencias.Agentes.Tail_Chasser()\n",
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "filler = dependencias.Agentes.Filler()\n",
    "\n",
    "filete = dependencias.Agentes.Combined_agent((           agente_avoider,\n",
    "                                                         agente_chaser,\n",
    "                                                         agente_searcher,\n",
    "                                                         agente_tail_chasser,\n",
    "                                                         filler),\n",
    "                                                        \n",
    "                                                        (1, 0.3, 2, 0.5, 0.3))\n",
    "\n",
    "game = dependencias.Snake_game((15, 15), 5, filete)"
   ],
   "id": "54709caef993dce",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:49:16.688655Z",
     "start_time": "2025-01-17T21:48:05.098267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "bb43836548fe60b9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T21:45:45.714972Z",
     "start_time": "2025-01-17T21:45:16.105183Z"
    }
   },
   "cell_type": "code",
   "source": "dependencias.enfrentar(filete, deep_loopy_looper, n_partidas=10)",
   "id": "d8abb18c468723f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Estadisticas          |    Agente 1     |    Agente 2    \n",
      "------------------------------ | --------------- | ---------------\n",
      "puntuacion_media               |   ((*97.0*))    |      87.6      \n",
      "puntuacion_maxima              |       110       |    ((*115*))   \n",
      "puntuacion_minima              |    ((*74*))     |       67       \n",
      "movimientos_medios             |   ((*911.9*))   |      622.6     \n",
      "movimientos_maximos            |   ((*1196*))    |       845      \n",
      "movimientos_minimos            |    ((*706*))    |       454      \n",
      "movimientos por puntuacion     |        9        |     ((*7*))    \n",
      "proporcion_del_tablero_ocupada |   ((*0.43*))    |      0.39      \n",
      "------------------------------ | --------------- | ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Parece que filete se comporta mejor que deep_loopy_looper, pero de nuevo da problemas con tema bucles y conflictos con chasser, por lo tanto lo siguiente que haremos es implementar una politica para evitar ciclos.\n",
    "\n",
    "<img src=\"media/imagenes/filete.png\" style=\"display: block; margin: auto;\" width=\"200\">\n",
    "\n",
    "*Lo que si que es muy prometedor es observar como se comporta el agente, claramente es campaz de gestionar mejor el espacio.*"
   ],
   "id": "7781fa7f0d738349"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cycle_detector\n",
    "\n",
    "Esta politica no hace nada hasta que detecta que la serpiente ha completado un ciclo identico (dos subsecuecias identicas en las secuencias de movimientos), a continuacion añadimos premios con el objetivo de salir del ciclo si ocurre dejaremos de premiar de nuevo. Una vez se detecta el bucle tenemos dos maneras de continuar:\n",
    "\n",
    "- (matar al agente) -> Premiando cualquier movimiento el agente termina movimiento, esto nos permite diseñar estrategias con riesgo de quedarse en bucle, y que no se queden en bucle, sin embargo dara lugar a partidas atipicas a las que se llegue a un bucle al principio y el agente muera.\n",
    "\n",
    "- (premiar searcher y añadir aleatoriedad) -> Hemos observado que searcher es eficaz saliendo de bucles ya que nos llevara a una casilla que aparentemente sera segura, esto es mas costoso, veremos si merece la pena."
   ],
   "id": "b0807fd4ca377a06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:18:51.026964Z",
     "start_time": "2025-01-17T22:18:51.023044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dependencias\n",
    "\n",
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "agente_chaser = dependencias.Agentes.ChaserAgent()\n",
    "agente_tail_chasser = dependencias.Agentes.Tail_Chasser()\n",
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "filler = dependencias.Agentes.Filler()\n",
    "cycle_detector = dependencias.Agentes.Cycle_detector()\n",
    "\n",
    "fileton = dependencias.Agentes.Combined_agent((agente_avoider,\n",
    "                                              agente_chaser,\n",
    "                                              agente_searcher,\n",
    "                                              agente_tail_chasser,\n",
    "                                              filler,\n",
    "                                              cycle_detector),\n",
    "\n",
    "                                             (2, 1, 0, 5, 0.1, 1))\n",
    "\n",
    "game = dependencias.Snake_game((15, 15), 5, filete)"
   ],
   "id": "41a37b6e6137af07",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:18:54.138649Z",
     "start_time": "2025-01-17T22:18:52.340829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "e40358aecdee0c3b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:19:01.651639Z",
     "start_time": "2025-01-17T22:18:56.715973Z"
    }
   },
   "cell_type": "code",
   "source": "game.evaluar(3)",
   "id": "196160376f3f36e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'puntuacion_media': np.float64(90.33),\n",
       " 'puntuacion_maxima': np.int64(103),\n",
       " 'puntuacion_minima': np.int64(84),\n",
       " 'movimientos_medios': np.float64(820.67),\n",
       " 'movimientos_maximos': np.int64(1031),\n",
       " 'movimientos_minimos': np.int64(695),\n",
       " 'movimientos por puntuacion': 9,\n",
       " 'proporcion_del_tablero_ocupada': np.float64(0.4)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:14:12.888889Z",
     "start_time": "2025-01-17T22:14:12.885909Z"
    }
   },
   "cell_type": "code",
   "source": "print()",
   "id": "b563588b37497f79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54a0bcd32acfac84"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
