{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><img src=\"media/imagenes/Banner.png\" height = 100 width = 1000></center>",
   "id": "d962182cfdf723d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Este notebooj pretende enseñar el resultadod de todas las implementaciones sin meterse mucho en los tedalles de la implementación, pretende ser una guia visual e interactiva, para endender todas las decisiones de diseño que hemos tomado en el proyecto. Esta pensado para ser ejecutado celda a celda salvo que se diga lo contrario.",
   "id": "e0375d750347b858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:20:43.437729Z",
     "start_time": "2025-01-17T18:20:43.225752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dependencias\n",
    "from pprint import pprint\n",
    "\n",
    "from dependencias import enfrentar"
   ],
   "id": "111b71fb835de854",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reglas\n",
    "\n",
    "Hay que enseñar lo que hemos implentado del juego base"
   ],
   "id": "22f54753c31fe610"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agentes",
   "id": "bc289102eebca739"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"media/imagenes/euristicos.png\" style=\"float: right;\">\n",
    "\n",
    "### Agentes Basados en Heuristicas.\n",
    "\n",
    "Los agentes que vamos a enseñar en este apartado son agentes que se basan en una politica de recompensa para decidir que movimiento realizar en cada turno, estan implementados mediante una clase abstracta que tiene los siguientes metodos:\n",
    "\n",
    "- get_reward: Calcula la recompensa asociada a cada movimiento en funcion de una politica de recompensa.\n",
    "- get_action: Calcula la mejor acción a realizar en función de la recompensa obtenida.\n",
    "\n",
    "de este modo podemos empezar a jugar con el agente y ver como se comporta.\n",
    "\n"
   ],
   "id": "4ebbbd6af6c0c68d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuacion vamos a enseñar los agentes que hemos implementado y como se comportan en el juego.",
   "id": "7acd514c49b15b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Chaser\n",
    "\n",
    "La politica de recompensa de este agente se basa en la distancia entre la cabeza de la serpiente y la comida mas cercana, de este modo el agente siempre intentará acercarse a la comida, aunque esto implique que la serpiente se muera.\n",
    "\n",
    "<center><img src=\"media/imagenes/chasser.png\" style=\"float: center;\"></center>"
   ],
   "id": "3e9d7108e737e3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:20:47.513051Z",
     "start_time": "2025-01-17T18:20:47.510202Z"
    }
   },
   "cell_type": "code",
   "source": "agente_chaser = dependencias.Agentes.ChaserAgent()",
   "id": "ce71e919a9774741",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:20:50.335600Z",
     "start_time": "2025-01-17T18:20:48.025592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente puedes modificar los parametros del juego para ver como se comporta en distintos escenarios\n",
    "\n",
    "SIZE = (15, 15)\n",
    "N_FOODS = 5\n",
    "\n",
    "game = dependencias.Snake_game(SIZE, N_FOODS, agente_chaser)\n",
    "game.play_with_pygame()"
   ],
   "id": "8b93e851f276b71d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:20:53.200137Z",
     "start_time": "2025-01-17T18:20:53.122082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ],
   "id": "a40361a9bfbb8600",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1511.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': 4,\n",
      " 'movimientos_maximos': np.int64(123),\n",
      " 'movimientos_medios': np.float64(32.28),\n",
      " 'movimientos_minimos': np.int64(7),\n",
      " 'proporcion_del_tablero_ocupada': 0.04,\n",
      " 'puntuacion_maxima': np.int64(26),\n",
      " 'puntuacion_media': np.float64(7.23),\n",
      " 'puntuacion_minima': np.int64(3)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "Este agente logra su objetivo de llegar muy rapido a por la comida (movimientos por puntuacion), pero no consigue sobrevivir mucho tiempo.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "Este agente podría ser mejorado si se le añade una penalización por muerte inmediata."
   ],
   "id": "982affadbdb6076b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "11254478ce44f5cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Avoider\n",
    "\n",
    "La politica de recompensa de este agente se basa en evitar las muertes inediatas, ya sea contra la serpriente o contra las paredes, asociando una penalización a estos movimientos.\n",
    "\n",
    "<center><img src=\"media/imagenes/inminent.png\" style=\"float: center;\"></center>"
   ],
   "id": "5895a96839b301c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:20:57.653282Z",
     "start_time": "2025-01-17T18:20:57.650285Z"
    }
   },
   "cell_type": "code",
   "source": "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()",
   "id": "47d1db2bdeefc94d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:03.756051Z",
     "start_time": "2025-01-17T18:20:58.158576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "SIZE = (15, 15)\n",
    "N_FOODS = 5\n",
    "\n",
    "game = dependencias.Snake_game(SIZE, N_FOODS, agente_avoider)\n",
    "game.play_with_pygame()"
   ],
   "id": "34242dee54cd21fe",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:04.070922Z",
     "start_time": "2025-01-17T18:21:03.806054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ],
   "id": "3161d72e0bd1f13b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 385.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': 61,\n",
      " 'movimientos_maximos': np.int64(1191),\n",
      " 'movimientos_medios': np.float64(600.67),\n",
      " 'movimientos_minimos': np.int64(195),\n",
      " 'proporcion_del_tablero_ocupada': 0.03,\n",
      " 'puntuacion_maxima': np.int64(18),\n",
      " 'puntuacion_media': np.float64(9.81),\n",
      " 'puntuacion_minima': np.int64(5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "Este agente majora mucho su tiempo de vida, pero no consigue alcanzar la comida de manera eficiente.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "Ademas vemos que con evitar muertes inmediatas no es suficiente, puesto que hay escenarios en los que te puedes quedar encerrado."
   ],
   "id": "4ce8684424ccb852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Combinacion de agentes\n",
    "\n",
    "Esta implementation de los agentes tan particular, nos permite combinar las recompensas que le dan a cada movimiento distintos agentes para conseguir un agente que se comporte de una manera más eficiente.\n",
    "\n",
    "<img src=\"media/imagenes/combinado.png\" style=\"display: block; margin: auto;\" width=\"400\">\n",
    "\n"
   ],
   "id": "b27f538b5d98c989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:10.081602Z",
     "start_time": "2025-01-17T18:21:10.078910Z"
    }
   },
   "cell_type": "code",
   "source": "chouder = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser), (1, 0.5))",
   "id": "26314a8eefae079a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:12.887492Z",
     "start_time": "2025-01-17T18:21:10.510423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "game = dependencias.Snake_game((15, 15), 5, chouder)\n",
    "game.play_with_pygame()"
   ],
   "id": "fed9b2ad778e7e8f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:13.453380Z",
     "start_time": "2025-01-17T18:21:12.921048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ],
   "id": "d6df7ca7bbbc7ec1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 189.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': 5,\n",
      " 'movimientos_maximos': np.int64(378),\n",
      " 'movimientos_medios': np.float64(193.22),\n",
      " 'movimientos_minimos': np.int64(65),\n",
      " 'proporcion_del_tablero_ocupada': 0.12,\n",
      " 'puntuacion_maxima': np.int64(68),\n",
      " 'puntuacion_media': np.float64(35.98),\n",
      " 'puntuacion_minima': np.int64(12)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "El comportaiento de este agente es mucho mejor que el de los agentes individuales, y parece una buena estrategia para el inicio del juego.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "El agente sigue sin ser capaz de evitar quedarse encerrado, hay que añadir una heuristica mas a futuro para evitar este tipo de situaciones.\n",
    "\n",
    "<img src=\"media/imagenes/encerrado.png\" style=\"display: block; margin: auto;\" width=\"400\">\n",
    "\n",
    "*en el ejemplo moverse a la derecha provoca una muerte inevitable pero el agente no lo considera una mala jugada*"
   ],
   "id": "b6b8c6818edb8ffc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Politicas basadas en busqueda en anchura (Estrategias a largo plazo)",
   "id": "31cd4850e427b219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "para hacer decisiones a largo plazo se nos plantean dos opciones.\n",
    "\n",
    "- Fuerza bruta (Exploracion en estados): podemos ver a donde nos lleva una combinacion de movimientos y ver cual es mejor.\n",
    "\n",
    "problema: el espacio de estados es muy grande y no podemos explorar todas las posibilidades, tendriamos que seleccionar una profundidad maxima y explorar hasta ahi. Además relentizaria cada simulacion.\n",
    "\n",
    "- Busqueda en anchura: Lanzando busqueda en anchura desde los movimientos posibles en cada turno podemos ver cual es el mejor movimiento a largo plazo, evaluando algunas caracteristicas del tablero.\n",
    "\n",
    "A continuacion desarrollamos agentes basados en busqueda en anchura, para solucionar probleas de muerte a largo plazo."
   ],
   "id": "6f563d3c81d96838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentes Searcher\n",
    "\n",
    "Este agente lanza 3 busquedas en anchura, empezando en cada una las casillas accedibles en el siguiente movimiento, aprovecha para contar el numer de casillas accesibles en cada caso y asigna una puntuacion a cada movimiento en funcion de este valor.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "$$\n",
    "W = \\frac{\\text{casillas accesibles en el movimiento } m}{\\text{total casillas accesibles}}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "*De este modo premiamos los movimientos que te llevan a los espacios abiertos*\n"
   ],
   "id": "35d3da55534f9a1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:16.457107Z",
     "start_time": "2025-01-17T18:21:16.453592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_searcher)"
   ],
   "id": "a07b8be939832686",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:19.677845Z",
     "start_time": "2025-01-17T18:21:17.092748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "ef29910c0ec2747a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuacion definimos una mezcla de los 3 agentes anteriores, para ver como se comporta en el juego.",
   "id": "586ee29055397e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:20.861505Z",
     "start_time": "2025-01-17T18:21:20.857754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agente_searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "agente_chaser = dependencias.Agentes.ChaserAgent()\n",
    "\n",
    "deep_chouder = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser, agente_searcher), (1, 0.2, 3))\n",
    "game = dependencias.Snake_game((15, 15), 5, deep_chouder)"
   ],
   "id": "bff528cebd91a5c6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:21:25.068193Z",
     "start_time": "2025-01-17T18:21:22.390449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "\n",
    "game.play_with_pygame()"
   ],
   "id": "3603921eb0d7aed3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Comportamiento del agente\n",
    "\n",
    "- El comportamiento de este agente es muy bueno, consigue sobrevivir mucho tiempo y llegar a la comida de manera eficiente, además cuando se queda encerrado sigue opcupando espacio hasta generar una salida y una vez existe es capaz de escapar generando muchas huidas interesantes."
   ],
   "id": "685473e6676d911f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-17T18:24:03.137877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lo comparamos con el agente que desarrollamos antes.\n",
    "# Esta celda puede tardar un poco en ejecutarse se recomienda siplemente mirar los resultados.\n",
    "\n",
    "dependencias.enfrentar(deep_chouder, chouder, n_partidas=100)"
   ],
   "id": "c16e1f8b7580c4ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Estas estaditicas muestran que deep_chouder es un agente mucho mejor que chouder, consigue sobrevivir mucho mas tiempo y llegar a mas comida. Sin embargo logicamente chouder es as eficiente y crece mas rapido.",
   "id": "59d939c67c157f79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Politicas basadas en busqueda en anchura (Estrategias a largo plazo)",
   "id": "8e797ac5ff7308c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "413c962dc7316050"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.767689100Z",
     "start_time": "2025-01-17T17:06:19.276223Z"
    }
   },
   "cell_type": "code",
   "source": "import dependencias",
   "id": "93effaff432b3c9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.767689100Z",
     "start_time": "2025-01-17T17:06:19.633045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tail_chasser = dependencias.Agentes.Tail_Chasser()\n",
    "avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "chasser = dependencias.Agentes.ChaserAgent()\n",
    "searcher = dependencias.Agentes.Busqueda_anchura()\n",
    "filleter = dependencias.Agentes.Filler()\n",
    "cycle_detector = dependencias.Agentes.Cycle_detector(avoid_skipable_loops=False)"
   ],
   "id": "e06bd650f94b4bba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.767689100Z",
     "start_time": "2025-01-17T17:06:21.350679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "combinado = dependencias.Agentes.Combined_agent(agentes = (tail_chasser, chasser, avoider, filleter, searcher, cycle_detector), weights= (10, 0.5, 0.55, 0.05, 0, 100))\n",
    "game = dependencias.Snake_game((15, 15), 5, combinado)"
   ],
   "id": "74e293e3ab58965f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.767689100Z",
     "start_time": "2025-01-17T16:53:49.392776Z"
    }
   },
   "cell_type": "code",
   "source": "game.play_with_pygame()",
   "id": "6d275279814064b0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.775209600Z",
     "start_time": "2025-01-17T17:03:36.787569Z"
    }
   },
   "cell_type": "code",
   "source": "game.evaluar(100)",
   "id": "2fac16f74c3c7762",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluar\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\practica_3_ALN\\dependencias\\juego_base.py:228\u001B[0m, in \u001B[0;36mSnake_game.evaluar\u001B[1;34m(self, n_partidas)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mis_game_over \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# Asegúrate de que el estado del juego se reinicie correctamente\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mis_game_over:\n\u001B[1;32m--> 228\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mupdate(action)\n\u001B[0;32m    230\u001B[0m puntuaciones\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39msnake))\n",
      "File \u001B[1;32m~\\Desktop\\practica_3_ALN\\dependencias\\Agentes\\Heuristic_Agent.py:21\u001B[0m, in \u001B[0;36mHeuristic_Agent.get_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Este metodo devuelve la accion a tomar en funcion del estado actual.\"\"\"\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Devolvemos la accion con mayor peso en caso de empate uno al azar de los que tengan el mayor peso\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_rewards\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m max_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m     25\u001B[0m best_actions \u001B[38;5;241m=\u001B[39m [action \u001B[38;5;28;01mfor\u001B[39;00m action, reward \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;241m==\u001B[39m max_reward]\n",
      "File \u001B[1;32m~\\Desktop\\practica_3_ALN\\dependencias\\Agentes\\Combined_Heuristic_Agent.py:27\u001B[0m, in \u001B[0;36mCombined_agent.get_rewards\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magentes]\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, agente \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magentes):\n\u001B[1;32m---> 27\u001B[0m     \u001B[43magente\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_rewards\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m accion \u001B[38;5;129;01min\u001B[39;00m agente\u001B[38;5;241m.\u001B[39mrewards:\n\u001B[0;32m     29\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards[accion] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights_list[i] \u001B[38;5;241m*\u001B[39m agente\u001B[38;5;241m.\u001B[39mrewards[accion]\n",
      "File \u001B[1;32m~\\Desktop\\practica_3_ALN\\dependencias\\Agentes\\Tail_Chasser.py:59\u001B[0m, in \u001B[0;36mTail_Chasser.get_rewards\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m start_pos, movimiento \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(lista_start_pos, MOVIMIENTOS):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m start_pos[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m state\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m start_pos[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m<\u001B[39m state\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m (start_pos \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m state\u001B[38;5;241m.\u001B[39msnake[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])):\n\u001B[1;32m---> 59\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtail_reachable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_pos\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     60\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards[movimiento] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m REWARD\n",
      "File \u001B[1;32m~\\Desktop\\practica_3_ALN\\dependencias\\Agentes\\Tail_Chasser.py:17\u001B[0m, in \u001B[0;36mTail_Chasser.get_rewards.<locals>.tail_reachable\u001B[1;34m(start_pos)\u001B[0m\n\u001B[0;32m     14\u001B[0m POSIBLES_ACCIONES \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)]\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Búsqueda en anchura para calcular las casillas accesibles desde start_pos\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m visited \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m queue \u001B[38;5;241m=\u001B[39m [start_pos]\n\u001B[0;32m     19\u001B[0m visited\u001B[38;5;241m.\u001B[39madd(start_pos)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.776211Z",
     "start_time": "2025-01-17T17:06:24.670932Z"
    }
   },
   "cell_type": "code",
   "source": "dependencias.enfrentar(combinado, chasser, n_partidas=2)",
   "id": "e12073f71b87a1be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2003.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Estadisticas          |    Agente 1     |    Agente 2    \n",
      "------------------------------ | --------------- | ---------------\n",
      "puntuacion_media               |   ((*221.0*))   |       6.0      \n",
      "puntuacion_maxima              |    ((*222*))    |        7       \n",
      "puntuacion_minima              |    ((*220*))    |        5       \n",
      "movimientos_medios             |  ((*3328.0*))   |      25.0      \n",
      "movimientos_maximos            |   ((*3448*))    |       32       \n",
      "movimientos_minimos            |   ((*3208*))    |       18       \n",
      "movimientos por puntuacion     |    ((*15*))     |        4       \n",
      "------------------------------ | --------------- | ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:04:11.776211Z",
     "start_time": "2025-01-17T15:47:04.145779Z"
    }
   },
   "cell_type": "code",
   "source": "game.evaluar(100)",
   "id": "4b0ac04abb73e651",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:36<00:00,  2.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'puntuacion_media': np.float64(208.62),\n",
       " 'puntuacion_maxima': np.int64(222),\n",
       " 'puntuacion_minima': np.int64(16),\n",
       " 'movimientos_medios': np.float64(3515.48),\n",
       " 'movimientos_maximos': np.int64(6807),\n",
       " 'movimientos_minimos': np.int64(110),\n",
       " 'movimientos por puntuacion': np.float64(16.851116863196243)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
