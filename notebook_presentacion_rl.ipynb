{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d962182cfdf723d6",
   "metadata": {},
   "source": [
    "<center><img src=\"media/imagenes/Banner.png\" height = 400 width = 1000></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111b71fb835de854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T10:29:43.055274Z",
     "start_time": "2025-01-14T10:29:42.750563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from sys import implementation\n",
    "\n",
    "import dependencias\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f54753c31fe610",
   "metadata": {},
   "source": [
    "### Reglas\n",
    "\n",
    "Hay que enseñar lo que hemos implentado del juego base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc289102eebca739",
   "metadata": {},
   "source": [
    "## Agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbbd6af6c0c68d",
   "metadata": {},
   "source": [
    "### Agentes Basados en Heuristicas.\n",
    "\n",
    "Hemos definido un primer tipo de agente basado en heurísticas que se basa en los siguientes pasos:\n",
    "\n",
    "- get_reward: Calcula la recompensa para cada movimiento en funcion de una politica de recompensa.\n",
    "- get_action: Calcula la mejor acción a realizar en función de la recompensa obtenida.\n",
    "\n",
    "de este modo podemos empezar a jugar con el agente y ver como se comporta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d7108e737e3f5",
   "metadata": {},
   "source": [
    "## Agentes Chaser\n",
    "\n",
    "La politica de recompensa de este agente se basa en la distancia entre la cabeza de la serpiente y la comida, de este modo el agente siempre intentará acercarse a la comida, aunque esto implique que la serpiente se muera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce71e919a9774741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T00:13:45.820784Z",
     "start_time": "2025-01-14T00:13:45.817770Z"
    }
   },
   "outputs": [],
   "source": [
    "agente_chaser = dependencias.Agentes.ChaserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b93e851f276b71d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T23:08:54.374752Z",
     "start_time": "2025-01-13T23:08:52.179042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_chaser)\n",
    "game.play_with_pygame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a40361a9bfbb8600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T23:12:09.371202Z",
     "start_time": "2025-01-13T23:12:09.309544Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1775.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': np.float64(4.296235679214402),\n",
      " 'movimientos_maximos': np.int64(115),\n",
      " 'movimientos_medios': np.float64(26.25),\n",
      " 'movimientos_minimos': np.int64(5),\n",
      " 'puntuacion_maxima': np.int64(23),\n",
      " 'puntuacion_media': np.float64(6.11),\n",
      " 'puntuacion_minima': np.int64(3)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982affadbdb6076b",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Este agente tarda solo 4 movimientos en conseguir la comida, pero no alcanza puntuaciones muy altas.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "Este agente podría ser mejorado si se le añade una penalización por muerte inmediata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11254478ce44f5cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5895a96839b301c5",
   "metadata": {},
   "source": [
    "## Agentes Avoider\n",
    "\n",
    "La politica de recompensa de este agente se basa en evitar las muertes inediatas, ya sea contra la serpriente o contra las paredes, asociando una penalización a estos movimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d1db2bdeefc94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T00:13:48.638862Z",
     "start_time": "2025-01-14T00:13:48.635482Z"
    }
   },
   "outputs": [],
   "source": [
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34242dee54cd21fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T23:15:34.687827Z",
     "start_time": "2025-01-13T23:14:45.679318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_avoider)\n",
    "game.play_with_pygame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3161d72e0bd1f13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T23:15:44.868171Z",
     "start_time": "2025-01-13T23:15:44.625383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 421.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': np.float64(59.01053740779768),\n",
      " 'movimientos_maximos': np.int64(1268),\n",
      " 'movimientos_medios': np.float64(560.01),\n",
      " 'movimientos_minimos': np.int64(91),\n",
      " 'puntuacion_maxima': np.int64(17),\n",
      " 'puntuacion_media': np.float64(9.49),\n",
      " 'puntuacion_minima': np.int64(4)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8684424ccb852",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Este agente majora mucho su tiempo de vida, pero no consigue alcanzar la comida de manera eficiente.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "Ademas vemos que con evitar muertes inmediatas no es suficiente, puesto que hay escenarios en los que te puedes quedar encerrado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f538b5d98c989",
   "metadata": {},
   "source": [
    "## Combinacion de agentes\n",
    "\n",
    "Esta implementation de los agentes tan particular, nos perite combinar las recompensas que le dan a cada movimiento distintos agentes para conseguir un agente que se comporte de una manera más eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26314a8eefae079a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T00:10:10.073119Z",
     "start_time": "2025-01-14T00:10:09.832960Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agente_chaser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agente_combinado \u001b[38;5;241m=\u001b[39m dependencias\u001b[38;5;241m.\u001b[39mAgentes\u001b[38;5;241m.\u001b[39mCombined_agent((agente_avoider, \u001b[43magente_chaser\u001b[49m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agente_chaser' is not defined"
     ]
    }
   ],
   "source": [
    "agente_combinado = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser), (1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed9b2ad778e7e8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T10:34:00.357665Z",
     "start_time": "2025-01-14T10:33:24.789689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecuta esta celda para ver como se comporta el agente\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_combinado)\n",
    "game.play_with_pygame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6df7ca7bbbc7ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T23:24:16.325384Z",
     "start_time": "2025-01-13T23:24:16.090221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 440.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': np.float64(60.81171067738231),\n",
      " 'movimientos_maximos': np.int64(2610),\n",
      " 'movimientos_medios': np.float64(529.67),\n",
      " 'movimientos_minimos': np.int64(133),\n",
      " 'puntuacion_maxima': np.int64(16),\n",
      " 'puntuacion_media': np.float64(8.71),\n",
      " 'puntuacion_minima': np.int64(4)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estadisticas = game.evaluar()\n",
    "pprint(estadisticas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8c6818edb8ffc",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "El comportaiento de este agente es mucho mejor que el de los agentes individuales, y parece una buena estrategia para el inicio del juego.\n",
    "\n",
    "### Mejoras\n",
    "\n",
    "El agente sigue sin ser capaz de evitar quedarse encerrado, hay que añadir una heuristica mas a futuro para evitar este tipo de situaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bff528cebd91a5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T10:44:16.097935Z",
     "start_time": "2025-01-14T10:43:45.154487Z"
    }
   },
   "outputs": [],
   "source": [
    "agente_searcher = dependencias.Agentes.Aantiloop()\n",
    "agente_avoider = dependencias.Agentes.Avoid_inmediate_death()\n",
    "agente_chaser = dependencias.Agentes.ChaserAgent()\n",
    "\n",
    "agente_combinado = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser, agente_searcher), (1, 0, 10))\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_combinado)\n",
    "game.play_with_pygame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e61d47864b2b6f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T10:45:05.197097Z",
     "start_time": "2025-01-14T10:44:31.114411Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': np.float64(40.07934508816121),\n",
      " 'movimientos_maximos': np.int64(3628),\n",
      " 'movimientos_medios': np.float64(3182.3),\n",
      " 'movimientos_minimos': np.int64(2509),\n",
      " 'puntuacion_maxima': np.int64(98),\n",
      " 'puntuacion_media': np.float64(79.4),\n",
      " 'puntuacion_minima': np.int64(66)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estadisticas = game.evaluar(10)\n",
    "pprint(estadisticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da367cd76aab33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T10:46:29.782256Z",
     "start_time": "2025-01-14T10:46:27.714319Z"
    }
   },
   "outputs": [],
   "source": [
    "agente_combinado = dependencias.Agentes.Combined_agent((agente_avoider, agente_chaser, agente_searcher), (1, 0.2, 3))\n",
    "game = dependencias.Snake_game((15, 15), 5, agente_combinado)\n",
    "game.play_with_pygame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f167c2222d2eee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T10:50:07.886054Z",
     "start_time": "2025-01-14T10:49:10.498605Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movimientos por puntuacion': np.float64(6.8849013073570875),\n",
      " 'movimientos_maximos': np.int64(971),\n",
      " 'movimientos_medios': np.float64(537.16),\n",
      " 'movimientos_minimos': np.int64(294),\n",
      " 'puntuacion_maxima': np.int64(126),\n",
      " 'puntuacion_media': np.float64(78.02),\n",
      " 'puntuacion_minima': np.int64(48)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estadisticas = game.evaluar(100)\n",
    "pprint(estadisticas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25020d1c",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf92443",
   "metadata": {},
   "source": [
    "Queríamos investigar y profundizar en el área del Aprendizaje Reforzado porque nos parece muy interesante el hecho de que un agente aprenda a jugar sin darle realmente instrucciones sobre la dinámica del juego. El agente buscar maximizar una función de calidad y equilibrar los pesos de una red neuronal  para que la recompensa obtenida sea máxima en cada paso, todo esto mediante prueba/error y simulaciones del escenario. \n",
    "\n",
    "Específicamente en el juego de Snake creemos que puede tener una implementación bastante beginner-friendly para aprender en este campo, ya que el juego tiene unas premisas relativamente sencillas.\n",
    "\n",
    "Nos gustaría profundizar en cómo surge la idea de aprendizaje por refuerzo, introduciendo sus componentes más importantes para tener una visión más o menos intuitiva de cómo funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f83505",
   "metadata": {},
   "source": [
    "## 1 Teoría"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31889a93",
   "metadata": {},
   "source": [
    "### 1.1 Procesos de Decisión de Markov\n",
    "\n",
    "Los problemas de aprendizaje reforzado frecuentemente se estructuran como un Proceso de Decisión de Markov (o MDP). Este está compuesto por un conjunto de estados $S$, un conjunto de acciones $A$ y un conjunto de recompensas $R$.\n",
    "\n",
    "Un estado $s_t\\in S$ describe el entorno en un instante $t$ específico.\n",
    "Teniendo en cuenta ese estado, el agente selecciona una acción a tomar $a_t\\in A$, lo que nos da lo que podemos llamar como el par estado-acción $(s_t, a_t)$.\n",
    "\n",
    "Después de la acción $s_t$, vamos al paso ${t+1}$ y al estado $s_{t+1}$, en el que recibimos una recompensa $r_{t+1}\\in R$ por la acción $a_t$ tomada en el estado $s_t$. \n",
    "Podemos pensar en este proceso como una función que mapea los pares con las recompensas: \n",
    "$$f(s_t, a_t) = r_{t+1}$$\n",
    "\n",
    "Ejemplo con nuestro juego:\n",
    "\n",
    "<center><img src=\"media/imagenes/rl/rl-snake-0.png\" height = 200 width = 200> <img src=\"media/imagenes/rl/flecha.png\" height = 200 width = 200> <img src=\"media/imagenes/rl/rl-snake-1.png\" height = 200 width = 200> <figcaption></figcaption></center>\n",
    "\n",
    "La figura de la izquierda representa el estado $s_t$; viendo el entorno, el agente decide tomar la acción $a_t$ (ir hacia la derecha) y el estado cambia al $s_{t+1}$ (figura de la derecha). Como se come una manzana, la recompensa $r_{t+1}$ es $1$. Por lo que en este caso $f(s_t,a_t) = 1$.\n",
    "\n",
    "Esto se repite una vez obtenida la recompensa tantas veces como queramos. <center><img src=\"media/imagenes/rl/rl-diagram.png\"><figcaption>Diagrama del proceso MDP</figcaption> </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c962dc7316050",
   "metadata": {},
   "source": [
    "### 1.2 Retorno Esperado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f16de2",
   "metadata": {},
   "source": [
    "El retorno esperado $G$, es la suma de recompensas obtenidas desde el estado actual hasta el final de la partida. Para que las recompensas futuras tengan un menor impacto que las inmediatas, se usa un factor de descuento $0\\lt \\gamma \\lt 1$. Cuando está cerca de 1, el algoritmo valora más las recompensas a largo plazo y cuando está cerca de 0, el algoritmo valora casi exclusivamente las recompensas inmediatas. \n",
    "Esta función de retorno con el gamma se llama retorno descontado, su fórmula es: $$G_{t} = R_{t+1} + \\gamma R_{t+2} + {\\gamma}^2R_{t+3} + \\ldots = R_{t+1} + \\gamma G_{t+1}$$\n",
    "En el Snake, el agente debe aprender a sobrevivir y obtener comida el mayor tiempo posible. Si $\\gamma=1$, la serpiente se preocupará por mantener una trayectoria que le permita comer y evitar muertes incluso a largo plazo. Si γ es muy bajo, la serpiente se enfocará en los alimentos sin tener en cuenta si le lleva a la muerte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73c6f3",
   "metadata": {},
   "source": [
    "### 1.3 Políticas y funciones de valor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5e0c6",
   "metadata": {},
   "source": [
    "La política $\\pi$ es la estrategia que sigue el agente para decidir qué acción tomar en cada estado. \n",
    "\n",
    "Es la pieza clave del agente en el MDP.\n",
    "\n",
    "La función de valor $v_\\pi(s)$ indica qué tan bueno es un estado $s$ si seguimos la política $\\pi$. \n",
    "\n",
    "Finalmente, la función de acción-valor $q_\\pi(s, a)$ indica qué tan bueno es elegir una acción $a$ de un estado $s$ siguiendo la política $\\pi$. \n",
    "En resumen: da el valor de una acción  siguiendo  $\\pi$.\n",
    "Su fórmula es: $$q_\\pi(s, a) = E[G_t | s_t, a_t]$$ A este valor se le llama q-valor y es la función que vamos a maximizar en el entrenamiento de la red neuronal.\n",
    "\n",
    "En Snake, la política indicaría si, estando en un cierto lugar y teniendo la comida en determinada posición, conviene moverse hacia arriba para buscar ese alimento o desviarse para no chocar con el borde. Las funciones de valor permiten cuantificar esos estados y acciones de forma comparativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f2014",
   "metadata": {},
   "source": [
    "### 1.4 Políticas óptimas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6d8a5",
   "metadata": {},
   "source": [
    "A partir de ahora a los estados $s_t$ los denominaremos $s$ y los estados $s_{t+1}$ los denominamos $s'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430080ab",
   "metadata": {},
   "source": [
    "El objetivo final del aprendizaje por refuerzo es encontrar una política óptima $\\pi^*$\n",
    "que maximiza el retorno esperado. Dicho de otro modo, queremos encontrar la secuencia de decisiones (movimientos en Snake) que conduzca al mayor número de recompensas acumuladas a lo largo del tiempo.\n",
    "\n",
    "En Snake, una política óptima sería aquella que, dadas las posiciones de la serpiente y la comida, decida cómo moverse para alargar la vida y comer de la manera más consistente posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e318cf9",
   "metadata": {},
   "source": [
    "La política óptima tiene una función de acción-valor óptima $q^*_\\pi(s, a) = max_{\\pi}q_\\pi (s,a)$, que devuelve el mayor retorno esperado conseguible por cualquier política $\\pi$ para cualquier par estado-acción posible.\n",
    "\n",
    "Una propiedad fundamental de $q^*(s,a)$ es que satisface la ecuación de optimalidad de Bellman: $$q^*_{\\pi}(s, a) = E[R_{t+1} + \\gamma max_{a'}q^*(s',a')$$\n",
    "donde:\n",
    "- $R_{t+1}$ es la recompensa inmediata recibida al pasar del estado $s$ al estado $s′$ con la acción a (+1 si comemos una manzana, -1 si morimos, etc.);\n",
    "- $\\gamma$ es el factor de descuento que pondera la importancia de las recompensas futuras;\n",
    "- $max_{a'}q^*(s',a')$ selecciona la acción $a'$ que maximiza la función de acción-valor óptima en el siguiente estado alcanzado, $s'$.\n",
    "\n",
    "Esta describe cómo el valor óptimo en un estado se relaciona con los valores en los estados siguientes. \n",
    "\n",
    "En términos del snake, la interpretación es:\n",
    "\n",
    "Se recibe una recompensa inmediata por la acción elegida,\n",
    "se proyecta cuál podría ser el siguiente mejor valor de acción en el nuevo estado, tomando el mayor $q^*$ disponible.\n",
    "Se combina la recompensa actual con ese máximo valor futuro (pesado por γ).\n",
    "La idea es que la serpiente, al realizar cada movimiento, no solo evalúe la recompensa inmediata (comer o no comer), sino también cuánto podría ganar a largo plazo (seguir viva para comer más adelante). \n",
    "\n",
    "Conforme vaya haciendo cada movimiento va a ir actualizando los valores {q}, las acciones que maximicen $q^*$ acabarán reflejando la política óptima: la forma de moverse que, en conjunto, genere mayor retorno acumulado en el juego.\n",
    "\n",
    "Os preguntaréis cómo va a actualizarse $q^*$, aquí es dónde introduciremos el algoritmo que usaremos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036dcada",
   "metadata": {},
   "source": [
    "### 1.5 Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21a709",
   "metadata": {},
   "source": [
    "Es un método fundamental en el Aprendizaje por Refuerzo para aprender la función de acción-valor óptima, sin necesidad de una política estricta predefinida. Se considera un algoritmo fuera de política (off-policy) porque aprende de la mejor acción posible independientemente de la política actual empleada para explorar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf39927",
   "metadata": {},
   "source": [
    "El agente mantiene una tabla de valores $Q(s, a)$, donde cada entrada representa el valor estimado de realizar la acción $a$ en el estado $s$. Partimos de valores iniciales (0 o aleatorios) y, en cada paso, cuando el agente está en un estado $s$, toma una acción $a$, obtiene una recompensa $r$ y salta al estado siguiente $s'$. A partir de esa experiencia, ajusta la entrada $Q(s, a)$ usando la ecuación de actualización del Q-Learning:\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha [r + \\gamma max(Q(s', a')) − Q(s, a)], $$\n",
    "Donde $\\alpha$ es la tasa de aprendizaje.\n",
    "\n",
    "Así, para Snake, con una tabla $Q$ de todos los estados posibles, podríamos seleccionar la acción que maximiza $Q(s, a)$.\n",
    "Estos son los pasos para cada iteración:\n",
    "1. La serpiente observa su estado actual $s$ (posición de la cabeza, comida, etc.).\n",
    "2. Selecciona una acción $a$ (moverse arriba, abajo, izquierda o derecha) de acuerdo con alguna política de exploración (por ejemplo, $\\epsilon-greedy$).\n",
    "3. Cuando ejecuta la acción, recibe una recompensa inmediata $r'$.\n",
    "4. La serpiente pasa a un nuevo estado $s'$.\n",
    "5. Se actualiza la tabla $Q$ para $Q(s, a)$ usando la fórmula anterior, incorporando la recompensa y la estimación de $\\max_{a'} q(s', a')$.\n",
    "\n",
    "Al repetir este proceso de forma iterativa y explorando distintos movimientos, la tabla $Q$ converge hacia una aproximación de $q^*(s,a)$, reflejando la utilidad real de cada acción. \n",
    "\n",
    "De ese modo, en partidas futuras, si la serpiente se encuentra en un estado parecido, se guiará por los mejores valores de Q, evitando muros con mayor probabilidad y priorizando comer la manzana cuando resulte más valioso a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f14d38",
   "metadata": {},
   "source": [
    "### 1.6 Política de exploración $\\epsilon-greedy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfae5fa",
   "metadata": {},
   "source": [
    "Como hemos nombrado en el paso 1.5.2, al principio se usa una estrategia de exploración-explotación.\n",
    "\n",
    "Cuando usamos un algoritmo como Q-Learning, el agente necesita una manera de balancear entre:\n",
    "- Explorar (probar acciones nuevas o aparentemente menos prometedoras)\n",
    "- Explotar (escoger siempre la acción con el valor $Q$ más alto que se ha aprendido).\n",
    "\n",
    "Si el agente solo explota, corre el riesgo de quedarse con una política que no sea la mejor, porque no ha probado suficientes opciones. En cambio, si el agente explora demasiado, no aprovechará lo que ya ha aprendido y se comportará de forma aleatoria. Para solucionar esto, se emplea normalmente la sencilla estrategia $\\epsilon-greedy$:\n",
    "\n",
    "Con probabilidad $\\epsilon$, se elige una acción aleatoria (exploración).\n",
    "Con probabilidad $1 − \\epsilon$, se elige la acción que maximiza el valor $Q$ del estado actual (explotación).\n",
    "\n",
    "```\n",
    "p = random()\n",
    "if p < ε:\n",
    "    explorar\n",
    "else:\n",
    "    explotar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae60e86",
   "metadata": {},
   "source": [
    "Ajuste de $\\epsilon$ a lo Largo del Entrenamiento\n",
    "- Al principio, conviene tener un $\\epsilon$ grande (por ejemplo, 1.0 o 0.9). De este modo, la serpiente explora acciones distintas y se hace una idea variada de cómo reaccionar en cada situación.\n",
    "- Conforme transcurre el entrenamiento y la serpiente “aprende” acciones efectivas, se reduce $\\epsilon$ para que explote más a menudo los movimientos que maximizan $Q(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42d386",
   "metadata": {},
   "source": [
    "En Snake:\n",
    "- Al principio, la serpiente se moverá de forma bastante aleatoria, intentando arriba/abajo cuando tal vez la comida está claramente a la derecha. Esto abre la posibilidad de descubrir rutas inesperadas que resulten favorables a largo plazo.\n",
    "- Pasado un tiempo, a medida que $Q(s, a)$ converge, la serpiente aprende una preferencia clara por ciertas acciones en estados determinados (es decir, va derecho a la comida o gira en la dirección apropiada), y disminuimos $\\epsilon$ para que aproveche dicho conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f67ab0",
   "metadata": {},
   "source": [
    "### 1.7 Deep Q-Learning con Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c55877",
   "metadata": {},
   "source": [
    "En escenarios con muchos estados o representaciones más complejas (como un tablero grande en Snake, donde cada celda puede ocupar diferentes posiciones de la serpiente y comida), la tabla $Q$ se vuelve inmanejable. Aquí entra en juego el Deep Q-Learning (DQN), que sustituye la tabla $Q$ por una Red Neuronal, capaz de aproximar $q(s, a)$ incluso cuando el espacio de estados es muy amplio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cddf1",
   "metadata": {},
   "source": [
    "Arquitectura Básica de la DQN\n",
    "- Entrada: Se alimenta a la red con una representación del estado de Snake (por ejemplo, la posición de la serpiente, la ubicación de la comida y posibles colisiones).\n",
    "- Salida: La red produce un conjunto de valores $q(s, a)$ (uno por cada acción: arriba, abajo, izquierda, derecha).\n",
    "- Optimización: Ajustamos los pesos de la red para que la diferencia entre las predicciones de $q$ y los valores de destino (targets) sea mínima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112c292",
   "metadata": {},
   "source": [
    "Un problema con el aprendizaje directo de secuencias es que los datos están correlacionados temporalmente, lo que puede desestabilizar la red. Para solucionarlo, se guarda la experiencia en un buffer (Replay Buffer) que contiene transiciones de la forma $(s, a, r, s', partida terminada)$. Luego se toma un mini-lote aleatorio (batch) de ese buffer en cada paso de entrenamiento. De esta forma se “rompe” la correlación de las muestras, haciendo que el entrenamiento sea más estable y el agente reutiliza experiencias pasadas, extrayendo más información de cada jugada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b387cf",
   "metadata": {},
   "source": [
    "Entrenamiento con Deep Q-Learning:\n",
    "\n",
    "- El agente observa el estado actual. Selecciona una acción $a$ (usando la política $\\epsilon-greedy$), la serpiente se mueve, recibe una recompensa y pasa a un nuevo estado s'. \n",
    "- Se almacena la transición ($s, a, r, s′, partidaterminada$) en el Replay Buffer.\n",
    "- Periódicamente, se muestrean lotes aleatorios del Replay Buffer para entrenar la DQN. Para cada transición del lote, se calcula el valor, objetivo y se compara con la salida de la red.\n",
    "- Se realiza retropropagación del error y ajuste de los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769281d",
   "metadata": {},
   "source": [
    "Al repetir este proceso en miles (o millones) de pasos, la red aprende a aproximar de forma estable $q(s, a)$. Esto le permite a la serpiente reconocer patrones en el tablero y actuar con mayor precisión en situaciones que no serían factibles de enumerar en una tabla tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1b890",
   "metadata": {},
   "source": [
    "### 1.8 Última mejora: Red objetivo para estabilizar el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d7d89",
   "metadata": {},
   "source": [
    "Cuando se entrena una Red Neuronal Profunda para aproximar la función de acción-valor $q(s, a)$, es fundamental mantener la estabilidad en las actualizaciones de los pesos de la red. Sin una estrategia adecuada, las actualizaciones continuas pueden llevar a que la red oscile o diverja, impidiendo que el agente aprenda de manera efectiva. Para abordar este desafío, se introduce la Red Objetivo (Target Network)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3623bc",
   "metadata": {},
   "source": [
    "Es una copia de la red neuronal principal (Q Network) que se utiliza para calcular los valores de referencia (targets) durante el entrenamiento. La idea principal es proporcionar una función objetivo más estable al mantener los pesos de la Red Objetivo fijos por un número determinado de iteraciones, en lugar de actualizarlos continuamente como ocurre con la Red Principal.\n",
    "\n",
    "- Sin una Red Objetivo, la función objetivo $q(s, a)$ se basa en la misma red que está siendo optimizada. Esto puede llevar a una retroalimentación circular donde los cambios en la red afectan directamente a los targets, causando inestabilidad.\n",
    "- Al mantener fija la Red Objetivo por múltiples actualizaciones, se disminuyen las fluctuaciones en los valores objetivo, permitiendo que la Red Principal aprenda de manera más gradual y consistente.\n",
    "- Con una Red Objetivo estable, es más probable que el algoritmo de optimización converja hacia una solución óptima, ya que las actualizaciones de peso no son impulsadas por cambios simultáneos en la función objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03599c",
   "metadata": {},
   "source": [
    "Nuevo entrenamiento de Deep Q-Learning con Target Network:\n",
    "\n",
    "- Crear dos redes neuronales: Red Principal $ NN_{\\text{q-net}} $ y Red Objetivo $ NN_{\\text{target-net}} $ con los mismos pesos iniciales.\n",
    "\n",
    "- El agente juega Snake, eligiendo acciones basadas en la política $\\epsilon $-greedy derivada de $ NN_{\\text{q-net}} $.\n",
    "\n",
    "- Cada transición (estado, acción, recompensa, siguiente estado) se almacena en el Experience Replay Buffer.\n",
    "\n",
    "- Se toman muestras aleatorias del buffer y se calculan los targets utilizando la Red Objetivo.\n",
    "La Red Principal se entrena para minimizar la diferencia entre sus predicciones y los targets.\n",
    "\n",
    "- Cada cierto número de pasos $\\theta_{\\text{target-net}} \\leftarrow \\theta_{\\text{q-net}}$, actualizar los pesos de la Red Objetivo para que coincidan con los de la Red Principal.\n",
    "\n",
    "- Continuar iterando sobre los pasos anteriores hasta que la Red Principal aprenda una función $ q $ que guíe al agente hacia una política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e22e8",
   "metadata": {},
   "source": [
    "Utilidad de todo lo dado para entender cómo funciona:\n",
    "\n",
    "1. Procesos de Decisión de Markov (MDP): Proporciona la estructura matemática para modelar el entorno y las interacciones del agente.\n",
    "\n",
    "2. Retorno Esperado: Define la suma de recompensas futuras descontadas, guiando al agente para maximizar sus ganancias a largo plazo.\n",
    "\n",
    "3. Políticas y Funciones de Valor: Estrategias y medidas que determinan y evalúan las acciones del agente.\n",
    "\n",
    "4. Políticas Óptimas: La meta del agente es descubrir la mejor política que maximice el retorno esperado.\n",
    "\n",
    "5. Q-Learning: Algoritmo que aprende la función de acción-valor óptima mediante actualizaciones iterativas.\n",
    "\n",
    "6. Exploración vs. Explotación ($\\epsilon-greedy$): Estrategia para balancear la búsqueda de nuevas acciones con la explotación de las conocidas.\n",
    "\n",
    "7. Deep Q-Learning y Experience Replay: Utilización de redes neuronales profundas y buffers de experiencia para manejar espacios de estado complejos.\n",
    "\n",
    "8. Red Objetivo: Introducción de una segunda red neuronal para estabilizar el entrenamiento de la DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb768ac8",
   "metadata": {},
   "source": [
    "## 2 Nuestra implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dependencias\n",
    "from dependencias.Agentes.deep_q_agent import state_function_8n, state_function_15, plot_scores_function, train_rl_agent, state_big_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4be24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c08880cd",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
